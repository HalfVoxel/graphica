#version 450
#extension GL_KHR_shader_subgroup_ballot: enable

layout(local_size_x = 32, local_size_y = 32, local_size_z = 1) in;

layout (set=0, binding=0, rgba8) uniform coherent image2D image;
layout (set=0, binding=1, rgba32f) uniform coherent image2D temp_image;
layout (set=0, binding=2) uniform sampler s_brush;
layout (set=0, binding=3) uniform texture2D t_brush;

#define PRIM_BUFFER_LEN 64

struct Primitive {
    uvec2 origin_src;
    uvec2 origin_dst;
    uint start;
};

layout(std140, binding = 4)
buffer restrict u_primitives { Primitive primitives[]; };

layout(std140, binding = 5)
uniform Settings {
    int width_per_group;
    int height_per_group;
    int num_primitives;
    int size_in_pixels;
};

layout(std140, binding = 6) buffer counter {
    volatile int next_work_item;
    volatile int done_work_items;
};

layout(std430, binding = 7) buffer restrict outdata {
    int outbuffer[];
};

vec4 blend_over(vec4 a, vec4 b) {
    // A over B
    // TODO: Might need more numerically stable algorithm
    return (a * a.a + b * b.a * (1 - a.a)) / (a.a + b.a * (1 - a.a));
}

// A over B
// Assumes a and b are premultiplied by their alpha
vec4 blend_over_premultiplied(vec4 a, vec4 b) {
    // A over B
    return a + b * (1 - a.a);
}

uint hash(uint r) {
    r = r ^ 61 ^ r >> 16;
    r = r + (r << 3);
    r = r ^ r >> 4;
    r = r * 668265261;
    r = r ^ r >> 15;
    return r;
}

/// Adds random noise to the color assuming it is going to be stored in an 8-bit per channel unorm texture.
///
/// Note: no rounding is done by this function.
///
/// See https://en.wikipedia.org/wiki/Rounding#Stochastic_rounding
vec4 stochastic_dithering_u8(vec4 color, uint seed) {
    uint noise = hash(seed);
    vec4 noisev = vec4(uvec4((noise >> 24) & 255, (noise >> 16) & 255, (noise >> 8) & 255, (noise >> 0) & 255)) / 255.0;
    color += (noisev - 0.5) * 0.99 / 255.0;
    return color;
}

vec4 gamma_to_linear_approximate(vec4 color) {
    return vec4(color.rgb*color.rgb, color.a);
}

vec4 linear_to_gamma_approximate(vec4 color) {
    return vec4(sqrt(color.rgb), color.a);
}

void pass_a(int primitive_index, uvec2 work) {
    uvec2 origin_src = primitives[primitive_index].origin_src;
    origin_src += work;

    uvec2 origin_dst = primitives[primitive_index].origin_dst;
    origin_dst += work;

    bool start = primitives[primitive_index].start != 0;

    ivec2 prev_image = ivec2(((primitive_index+0) % 2) * size_in_pixels, 0);
    ivec2 next_image = ivec2(((primitive_index+1) % 2) * size_in_pixels, 0);

    vec4 a;
    if (start) {
        a = imageLoad(image, ivec2(origin_src));
        a = gamma_to_linear_approximate(a);
    } else {
        // Essentially the same as the line above, but this image has higher precision
        a = imageLoad(temp_image, ivec2(work) + prev_image);
    }
    vec4 b;
    ivec2 relative = ivec2(origin_dst) - ivec2(primitives[primitive_index].origin_src);
    if (!start && all(greaterThanEqual(relative, ivec2(0))) && all(lessThan(relative, ivec2(size_in_pixels)))) {
        // If we are inside the bounds of the high precision texture we can sample from that instead
        b = imageLoad(temp_image, relative + prev_image);
    } else {
        // Otherwise fall back to the low-precision image
        b = imageLoad(image, ivec2(origin_dst));
        b = gamma_to_linear_approximate(b);
    }

    vec2 uv = vec2(work) / vec2(size_in_pixels, size_in_pixels);
    vec4 c = texture(sampler2D(t_brush, s_brush), uv);

    float blend = c.a * 0.5;
    float out_alpha = mix(b.a, a.a, blend);
    out_alpha = 1.0;
    vec3 col = mix(b.rgb * b.a, a.rgb * a.a, blend) / out_alpha;
    vec4 result = vec4(col, out_alpha);
    imageStore(temp_image, ivec2(work) + next_image, result);
}

void pass_b(int primitive_index, uvec2 work) {
    uvec2 origin_dst = primitives[primitive_index].origin_dst;
    origin_dst += work;

    ivec2 next_image = ivec2(((primitive_index+1) % 2) * size_in_pixels, 0);

    if (primitive_index + 1 < num_primitives) {
        uvec2 mn = primitives[primitive_index+1].origin_dst;
        uvec2 mx = mn + uvec2(size_in_pixels, size_in_pixels);
        if (all(greaterThanEqual(origin_dst, mn)) && all(lessThan(origin_dst, mx))) {
            // This pixel will be written in the next step anyway, skip it
            return;
        }
    }
    return;

    vec4 a = imageLoad(temp_image, ivec2(work) + next_image);

    a = linear_to_gamma_approximate(a);

    // Random noise dithering
    uvec2 hashcoord = origin_dst + ivec2(primitive_index, 0);
    vec4 result = stochastic_dithering_u8(a, hashcoord.x*1024 + hashcoord.y);
    imageStore(image, ivec2(origin_dst), result);
}

void main() {
    // For each primitive
    //    ensure all previous work is done -> barrier
    //    atomic add to get new id
    //    perform work
    //    atomic add work done
    int chunk_size = size_in_pixels*size_in_pixels;
    while (true) {
        int val = atomicAdd(next_work_item, 1);

        int chunk = val / chunk_size;
        int primitive = chunk / 2;
        if (primitive >= num_primitives) {
            break;
        }

        int done_threshold = chunk*chunk_size;
        while (done_work_items < done_threshold) {
            // Without this barrier the loop will be optimized out
            memoryBarrier();
        }

        // Safeguard to make sure the above loop is not optimized out
        if (done_work_items < done_threshold) {
            break;
        }

        int wId = val % chunk_size;

        uvec2 work = uvec2(wId % size_in_pixels, wId / size_in_pixels);
        
        if ((chunk % 2) == 0) {
            pass_a(primitive, work);
        } else {
            pass_b(primitive, work);
        }

        memoryBarrier();
        atomicAdd(done_work_items, 1);
    }
}
